{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tests import generate_regression_data, test_regression_model, test_knn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация метода линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требования:\n",
    "\n",
    "* l1-регуляризация\n",
    "* l2-регуляризация\n",
    "* При обучении данные подаются в SGD с помощью итератора\n",
    "* Для нахождения весов используется стохастический градиентный спуск\n",
    "* Каждый шаг градиентного спуска использует ровно один обучающий пример\n",
    "* Стохастический градиентный спуск использует приближенно вычисленный градиент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Необходимо реализовать следующий (-ие) класс (-ы)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    '''Класс для предсказания действительно-значного выхода по входу - вектору из R^n. \n",
    "    Используется линейная регрессия, то есть если вход X \\in R^n, вектор весов W \\in R^{n+1},\n",
    "    то значение регрессии - это [X' 1] * W, то есть y = x1*w1 + x2*w2 + xn*wn + wn+1.\n",
    "    Обучение - подгонка весов W - будет вестись на парах (x, y).\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    sgd : объект класса SGD\n",
    "    trainiterator: объект класса TrainIterator\n",
    "    n_epoch : количество эпох обучения (default = 1)\n",
    "    '''\n",
    "    def __init__(self, sgd, trainiterator, n_epoch=1):\n",
    "        self.sgd = sgd\n",
    "        self.trainiterator = trainiterator\n",
    "        self.n_epoch = n_epoch\n",
    "     \n",
    "    def fit(self, X, y):\n",
    "        '''Обучение модели.\n",
    "        \n",
    "        Парметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        Метод обучает веса W\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        for a, b in self.trainiterator:\n",
    "            self.W = self.sgd.step(a, b, self.W)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Предсказание выходного значения для входных векторов\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        y : Массив размера n_samples\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        \n",
    "        return X @ self.W\n",
    "        \n",
    "    def score(self, y_gt, y_pred):\n",
    "        \"\"\"Возвращает точность регрессии в виде (1 - u/v),\n",
    "        где u - суммарный квадрат расхождения y_gt с y_pred,\n",
    "        v - суммарный квадрат расхождения y_gt с матожиданием y_gt\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        y_gt : массив/список правильных значений размера n_samples\n",
    "        y_pred : массив/список предсказанных значений размера n_samples\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        accuracy - точность регрессии\n",
    "        \"\"\"\n",
    "        y_gt = np.array(y_gt)\n",
    "        y_pred = np.array(y_pred)\n",
    "        \n",
    "        return 1 - sum((y_gt - y_pred) ** 2) / (sum((y_gt - y_gt.mean()) ** 2) if (y_gt.size > 1) else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Шаг градиентного спуска\n",
    "В функцию `step` передаётся набор признаков одного объекта (переменная `X`), правильное значение (переменная `y`), соответствующее этому объекту из тренировочной выборки, и массив весов (переменная `W`). Функция `step` возвращает обновлённые веса, вычисленные с использованием предыдущих значений весов `W` и градиента от функции потерь `self.grad.grad` по этим старым весам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    '''Класс для реализации метода стохастического градиентного спуска. \n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    grad : функция вычисления градиента\n",
    "    alpha : градиентный шаг (default = 1.)\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, grad, alpha=1.):\n",
    "        self.grad = grad\n",
    "        self.alpha = alpha\n",
    "     \n",
    "    def step(self, X, y, W):\n",
    "        '''Один шаг градиентного спуска.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает обновленные веса\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        W = np.array(W)\n",
    "        \n",
    "        return W - self.alpha * self.grad.grad(X, y, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Градиент\n",
    "Вычисляется приближенно, численно. В функцию `grad` передаётся набор признаков одного объекта (переменная `X`), правильное значение (переменная `y`) соответствующее этому объекту из тренировочной выборки, и массив весов (переменная `W`). При больших объёмах тренировочных данных затратным по памяти может быть использование итерирования по единичной матрице `np.eye(len(X)) * self.delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grad(object):\n",
    "    '''Класс для вычисления градиента по весам от функции потерь. \n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    loss : функция потерь\n",
    "    delta : параметр численного дифференцирования (default = 0.000001)    \n",
    "    '''\n",
    "    def __init__(self, loss, delta=0.000001):\n",
    "        self.loss = loss\n",
    "        self.delta = delta\n",
    "     \n",
    "    def grad(self, X, y, W):\n",
    "        '''Вычисление градиента.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает градиент по весам W в точках X от функции потерь\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        W = np.array(W)\n",
    "        \n",
    "        loss = self.loss.val(X, y, W)\n",
    "        gradient = list()\n",
    "        for d in np.eye(len(X)) * self.delta:\n",
    "            gradient.append((self.loss.val(X, y, W + d) - loss) / self.delta)\n",
    "        return np.array(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Функция потерь\n",
    "В функцию `val` передаётся набор признаков одного объекта (переменная `X`), правильное значение (переменная `y`), соответствующее этому объекту из тренировочной выборки, и массив весов (переменная `W`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''Класс для вычисления функции потерь. \n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    l1_coef : коэффициент l1 регуляризации (default = 0)\n",
    "    l2_coef : коэффициент l2 регуляризации (default = 0)\n",
    "    '''\n",
    "    def __init__(self, l1_coef=0, l2_coef=0):\n",
    "        self.l1 = l1_coef\n",
    "        self.l2 = l2_coef\n",
    "     \n",
    "    def val(self, X, y, W):\n",
    "        '''Вычисление функции потерь.\n",
    "        \n",
    "        Параметры\n",
    "        ----------\n",
    "        X : двумерный массив признаков размера n_samples x n_features\n",
    "        y : массив/список правильных значений размера n_samples\n",
    "        W : массив весов размера n_weights\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает значение функции потерь в точках X\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        W = np.array(W)\n",
    "        \n",
    "        return (W @ X - y) ** 2 + self.l1 * sum(abs(W)) + self.l2 * sum(W ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Итератор\n",
    "На каждой итерации возвращает пару: набор значений признаков одного объекта из X и соответствующее ему значение из y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainIterator(object):\n",
    "    '''Класс итератора для работы с обучающими данными. \n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    X : двумерный массив признаков размера n_samples x n_features\n",
    "    y : массив/список правильных значений размера n_samples\n",
    "    n_epoch : количество эпох обучения (default = 1)\n",
    "    '''    \n",
    "    def __init__(self, X, y, n_epoch=1):\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.n_epoch = n_epoch\n",
    "        self.i = -1\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''Нужно для использования итератора в цикле for\n",
    "        Здесь ничего менять не надо\n",
    "        '''\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        '''Выдача следующего примера.\n",
    "        \n",
    "        Выход\n",
    "        -------\n",
    "        Метод возвращает очередной пример как из X, так и из y\n",
    "        '''\n",
    "        self.i += 1\n",
    "        if self.i >= self.X.shape[0]:\n",
    "            self.i = 0\n",
    "            self.epoch += 1\n",
    "        if self.epoch >= self.n_epoch:\n",
    "            raise StopIteration\n",
    "        return self.X[self.i], self.y[self.i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример использования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.9999999996161648\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_train_samples = 1000\n",
    "n_test_samples = 500\n",
    "n_features = 15\n",
    "\n",
    "trainX = np.append(np.ones((n_train_samples, 1)), np.random.rand(n_train_samples, n_features), axis=1)\n",
    "W = np.random.rand(n_features + 1)\n",
    "trainY = trainX @ W\n",
    "\n",
    "testX = np.append(np.ones((n_train_samples, 1)), np.random.rand(n_train_samples, n_features), axis=1)\n",
    "testY = testX @ W\n",
    "\n",
    "n_epoch = 2\n",
    "l1_coef = 0.0000000001\n",
    "l2_coef = 0.0000000001\n",
    "delta = 0.000000001\n",
    "alpha = 0.13311\n",
    "\n",
    "trainiterator = TrainIterator(trainX, trainY, n_epoch)\n",
    "loss = Loss(l1_coef, l2_coef)\n",
    "grad = Grad(loss, delta)\n",
    "sgd = SGD(grad, alpha)\n",
    "reg = Regression(sgd, trainiterator, n_epoch)\n",
    "reg.fit(trainX, trainY)\n",
    "y_pred = reg.predict(testX)\n",
    "acc = reg.score(testY, y_pred)\n",
    "print('Accuracy is %s' % str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Покройте ваш класс тестами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Импортированный тест регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY, testX, testY = generate_regression_data(Nfeat=100, Mtrain=150, Mtest=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST REGRESSION MODEL: Your accuracy is 0.9525424573474773\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 7\n",
    "delta = 1e-07\n",
    "l1_coef = 4.166626769164239e-10\n",
    "l2_coef = -1.1334034790699833e-09\n",
    "alpha = 0.022233325294654372\n",
    "\n",
    "trainiterator = TrainIterator(trainX, trainY, n_epoch)\n",
    "loss = Loss(l1_coef, l2_coef)\n",
    "grad = Grad(loss, delta)\n",
    "sgd = SGD(grad, alpha)\n",
    "reg = Regression(sgd, trainiterator, n_epoch)\n",
    "\n",
    "test_regression_model(reg, trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подберите оптимальные параметры для вашей модели\n",
    "Здесь ниже указаны параметры для запуска на тесте `test_regression_model` и данных `generate_regression_data(Nfeat=100, Mtrain=150, Mtest=150)` при `np.random.seed(0)`.\n",
    "\n",
    "Если считать тройку (`l1_coef`, `l2_coef`, `alpha`) точкой `prm` в пространстве, а действия [обучение, предсказание, подсчёт точности] считать функцией `regr` (значением которой является accuracy), зависящей от параметров `l1_coef`, `l2_coef`, `alpha`, то можно итеративно вычислять градиент от `regr` в точке `prm`, двигать точку `prm` в направлении этого градиента, тем самым приближать функцию `regr` к наибольшему значению. В качестве начального приближения для `prm` выбирается точка (0, 0, 0).\n",
    "\n",
    "У этого способа также имеется собственный параметр `eps=1e-09`, шаг подъёма, выбранный из соображений, что большее 1e-08 и меньшее 1e-10 либо требуют очень много итераций для приближения, либо дают худшую точность.\n",
    "\n",
    "Функция `prm` ресурсозатратная, вычисляется долго, в зависимости от `n_epoch` и объёма тренировочных данных. За одну итерацию для сдвига точки `prm` функция `regr` вычисляется 4 раза. Варьирование параметров `delta`, `n_epoch` производится вручную, в то время как подбор параметров `l1_coef`, `l2_coef`, `alpha` производится вышеописанным \"градиентным подъёмом\" автоматически.\n",
    "\n",
    "#### Подбор `delta`\n",
    "При варьировании параметра `delta` в промежутке [1e-07, 1e-03] и при фиксированном `n_epoch` не наблюдалось значительного изменения score (разница составляла не более 0.00003).\n",
    "\n",
    "#### Подбор `n_epoch`\n",
    "При варьировании параметра `n_epoch` в диапазоне {1, ..., 8} при `delta=1e-07` были получены следующие точности:\n",
    "\n",
    "`n_epoch=8`, `score=0.9385284009811397`\n",
    "\n",
    "`n_epoch=7`, `score=0.9525424537366032`\n",
    "\n",
    "`n_epoch=6`, `score=0.9437277416335921`\n",
    "\n",
    "`n_epoch=5`, `score=0.9301497096183774`\n",
    "\n",
    "`n_epoch=4`, `score=0.9117210979400182`\n",
    "\n",
    "`n_epoch=3`, `score=0.8860629826647402`\n",
    "\n",
    "`n_epoch=2`, `score=0.8476816327041838`\n",
    "\n",
    "`n_epoch=1`, `score=0.7688739944173726` (требует, видимо, очень много времени или более мощной машины)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration score: -214.2945085757396\n",
      "iteration score: 0.9525424537366032\n",
      "final score: 0.9525424537366032\n",
      "l1_coef: 4.166626769164239e-10\n",
      "l2_coef: -1.1334034790699833e-09\n",
      "alpha: 0.022233325294654372\n",
      "delta: 1e-07\n",
      "n_epoch: 7\n"
     ]
    }
   ],
   "source": [
    "# Нужно подобрать alpha, delta, l1_coef, l2_coef, n_epoch для максимизации score\n",
    "\n",
    "delta = 1e-07\n",
    "n_epoch = 7\n",
    "\n",
    "\n",
    "def regr(prm):\n",
    "    trainiterator = TrainIterator(trainX, trainY, n_epoch)\n",
    "    loss = Loss(prm[0], prm[1])\n",
    "    grad = Grad(loss, delta)\n",
    "    sgd = SGD(grad, prm[2])\n",
    "    reg = Regression(sgd, trainiterator, n_epoch)\n",
    "    reg.fit(trainX, trainY)\n",
    "    y_pred = reg.predict(testX)\n",
    "    return reg.score(testY, y_pred)\n",
    "\n",
    "\n",
    "def gradient(prm, fixed):\n",
    "    grd = np.zeros(3)\n",
    "    for i, e in enumerate(np.eye(3) * 0.000001):\n",
    "        grd[i] = (regr(prm + e) - fixed) / 0.000001\n",
    "    return grd\n",
    "\n",
    "\n",
    "prm = np.array([0, 0, 0])\n",
    "score_old = 1\n",
    "score = 0\n",
    "eps = 1e-09\n",
    "\n",
    "while score < 0.8:\n",
    "    score = regr(prm)\n",
    "    prm = prm + gradient(prm, score) * eps\n",
    "    score_old = score\n",
    "    print('iteration score:', score)\n",
    "    \n",
    "print('final score:', score)\n",
    "print('l1_coef:', prm[0])\n",
    "print('l2_coef:', prm[1])\n",
    "print('alpha:', prm[2])\n",
    "print('delta:', delta)\n",
    "print('n_epoch:', n_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
